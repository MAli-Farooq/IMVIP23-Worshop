{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8923234",
   "metadata": {},
   "source": [
    "# Install StyleGAN-ADA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dvschultz/stylegan2-ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001612e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python==3.4.13.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da484be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls stylegan2-ada/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd stylegan2-ada\n",
    "!mkdir downloads\n",
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d35ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8629f1",
   "metadata": {},
   "source": [
    "# Download dataset and Convert dataset to .tfrecords\n",
    "\n",
    "1. Download Metfaces: https://github.com/NVlabs/metfaces-dataset\n",
    "   put the dataset into folder. e.g. training_session/datasets/Metfaces\n",
    "2. Other datasets could find in: https://github.com/NVlabs/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to download the required datsets and convert it to ensorflow formats i.e tf.records \n",
    "# Dowload dataset\n",
    "!wget \"https://www.dropbox.com/s/0ybsudabqscstf7/biked_dataset.tar.gz\" -q -O biked_dataset.tar.gz\n",
    "# extract dataset\n",
    "!tar -zxvf biked_dataset.tar.gz\n",
    "# Delete the tar.gz file\n",
    "!rm biked_dataset.tar.gz\n",
    "!mv biked ../datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "#update this to the path to your image folder\n",
    "dataset_path = \"../datasets/biked\" #\"../datasets/MetFaces\" #\"/home/ubuntu/Desktop/training_session/datasets/MetFaces\"\n",
    "#give your dataset a name\n",
    "dataset_name = 'biked' #'MetFaces'\n",
    "\n",
    "#you don't need to edit anything here\n",
    "!python dataset_tool.py create_from_images ./datasets/{dataset_name} {dataset_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bcfaa",
   "metadata": {},
   "source": [
    "# Training process\n",
    "\n",
    "## Train a custom model\n",
    "\n",
    "We’re ready to start training! There are numerous arguments to training, what’s listed below are the most popular options. To see all the options, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bde305",
   "metadata": {},
   "source": [
    "### transfer learning -- finetuning from a existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48259a85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this name must EXACTLY match the dataset name you used when creating the .tfrecords file\n",
    "dataset_name = \"MetFaces\" #\"mydataset\"\n",
    "#how often should the model generate samples and a .pkl file\n",
    "snapshot_count = 2\n",
    "#should the images be mirrored left to right?\n",
    "mirrored = True\n",
    "#should the images be mirrored top to bottom?\n",
    "mirroredY = True\n",
    "#metrics? \n",
    "metric_list = None\n",
    "#\n",
    "# this is the most important cell to update\n",
    "#\n",
    "# running it for the first time? set it to ffhq(+resolution)\n",
    "# resuming? get the path to your latest .pkl file and use that\n",
    "resume_from = \"ffhq1024\"\n",
    "\n",
    "#don't edit this unless you know what you're doing :)\n",
    "!python train.py --outdir ./results --snap={snapshot_count} --cfg=11gb-gpu --data=./datasets/{dataset_name} --mirror={mirrored} --mirrory={mirroredY} --metrics={metric_list} --resume={resume_from}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6b78a",
   "metadata": {},
   "source": [
    "# Generate images from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, HTML\n",
    "import base64\n",
    "import io\n",
    "\n",
    "def show_local_mp4_video(file_name, width=640, height=480):\n",
    "  video_encoded = base64.b64encode(io.open(file_name, 'rb').read())\n",
    "  return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n",
    "                        <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n",
    "                      </video>'''.format(width, height, video_encoded.decode('ascii')))\n",
    "def make_img_grid(images,width=360):\n",
    "    html = []\n",
    "    for image in images:\n",
    "        with open(image, \"rb\") as img_file:\n",
    "            my_string = base64.b64encode(img_file.read())\n",
    "            img_uri = \"data:image/png;base64,\" + my_string.decode('utf8')\n",
    "        html.append('<img src=\"{}\" style=\"width:{}px;display:inline;margin:1px\"/>'.format(img_uri,str(width)))\n",
    "    return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0422b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b077036",
   "metadata": {},
   "source": [
    "## Options\n",
    "`--network`: Make sure the `--network` argument points to your .pkl file.\n",
    "\n",
    "`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
    "\n",
    "`--trunc`: This sets the truncation amount.This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically it's infinite. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
    "\n",
    "\n",
    "`--outdir`:  Where to save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-images\\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl \\\n",
    "  --outdir=results --seeds=6600-6620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfImageNames = ['./results/seed6600.png',\n",
    "                    './results/seed6601.png',\n",
    "                    './results/seed6602.png',\n",
    "                    './results/seed6603.png',\n",
    "                    './results/seed6604.png',\n",
    "                    './results/seed6605.png',\n",
    "                    './results/seed6606.png',\n",
    "                    './results/seed6608.png',\n",
    "                    './results/seed6609.png',\n",
    "                    './results/seed6610.png',\n",
    "                    './results/seed6611.png',\n",
    "                    './results/seed6612.png',\n",
    "                    './results/seed6613.png',\n",
    "                    './results/seed6614.png',\n",
    "                    './results/seed6615.png',\n",
    "                    './results/seed6616.png',\n",
    "                    './results/seed6617.png',\n",
    "                   ]\n",
    "\n",
    "# for imageName in listOfImageNames:\n",
    "#     display(Image(filename=imageName, width=400))\n",
    "    \n",
    "\n",
    "display(HTML(make_img_grid(listOfImageNames, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-images \\\n",
    "    --outdir=out_child_girl --trunc=0.7 \\\n",
    "    --seeds=1010-1018 --create-grid \\\n",
    "    --network=\"./pretrain/Girls.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b0762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(Image(filename='./out_child_girl/grid.png', width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-images \\\n",
    "    --outdir=out_child_boy --trunc=0.7 \\\n",
    "    --seeds=1000-1008 --create-grid \\\n",
    "    --network=\"./pretrain/Boys.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='./out_child_boy/grid.png', width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08201085",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-images \\\n",
    "    --outdir=out_fish --trunc=0.7 \\\n",
    "    --seeds=1000-1008 --create-grid \\\n",
    "    --network=https://github.com/jeffheaton/pretrained-gan-fish/releases/download/1.0.0/fish-gan-2020-12-09.pkl\n",
    "\n",
    "#display(Image(filename='/home/ubuntu/Desktop/training_session/stylegan2-ada/out_fish/grid.png', width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e4ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='/home/ubuntu/Desktop/training_session/stylegan2-ada/out_fish/grid.png', width=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-images \\\n",
    "    --outdir=out_metfaces --trunc=0.7 \\\n",
    "    --seeds=1000-1008 --create-grid \\\n",
    "    --network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl\n",
    "    #https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/download/v1/christmas-gan-2020-12-03.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b504c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Image(filename='/home/ubuntu/Desktop/training_session/stylegan2-ada/out_metfaces/grid.png', width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c2bfc",
   "metadata": {},
   "source": [
    "# Latent space exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965345d",
   "metadata": {},
   "source": [
    "Transforming the latent vector between two images.\n",
    "You can create a video that shows the progression through two GAN seeds. This technique creates a very cool \"morph\" effect.\n",
    "![GAN](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/gan_progression.jpg \"GAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aef8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"stylegan2-ada\")\n",
    "\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "\n",
    "def seed2vec(Gs, seed):\n",
    "  rnd = np.random.RandomState(seed)\n",
    "  return rnd.randn(1, *Gs.input_shape[1:])\n",
    "\n",
    "def init_random_state(Gs, seed):\n",
    "  rnd = np.random.RandomState(seed) \n",
    "  noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
    "  tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) # [height, width]\n",
    "\n",
    "def display_image(image):\n",
    "  plt.axis('off')\n",
    "  plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "def generate_image(Gs, z, truncation_psi):\n",
    "    # Render images for dlatents initialized from random seeds.\n",
    "    Gs_kwargs = {\n",
    "        'output_transform': dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True),\n",
    "        'randomize_noise': False\n",
    "    }\n",
    "    if truncation_psi is not None:\n",
    "        Gs_kwargs['truncation_psi'] = truncation_psi\n",
    "\n",
    "    label = np.zeros([1] + Gs.input_shapes[1][1:])\n",
    "    images = Gs.run(z, label, **Gs_kwargs) # [minibatch, height, width, channel]\n",
    "    return images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d8f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_seed(seeds, vector_size):\n",
    "  result = []\n",
    "\n",
    "  for seed in seeds:\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    result.append( rnd.randn(1, vector_size) ) \n",
    "  return result\n",
    "\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-gan-fish/releases/download/1.0.0/fish-gan-2020-12-09.pkl\"\n",
    "#URL = \"https://github.com/jeffheaton/pretrained-merry-gan-mas/releases/download/v1/christmas-gan-2020-12-03.pkl\"\n",
    "URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl\"\n",
    "tflib.init_tf()\n",
    "print('Loading networks from \"%s\"...' % URL)\n",
    "with dnnlib.util.open_url(URL) as fp:\n",
    "    _G, _D, Gs = pickle.load(fp)\n",
    "\n",
    "vector_size = Gs.input_shape[1:][0]\n",
    "# range(8192,8300)\n",
    "seeds = expand_seed( [8192+1,8192+9], vector_size)\n",
    "#generate_images(Gs, seeds,truncation_psi=0.5)\n",
    "print(seeds[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your seeds to morph through and the number of steps to take to get to each.\n",
    "\n",
    "SEEDS = [3004,3031,3033,3111,3191,3253]\n",
    "STEPS = 100\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Remove any prior results\n",
    "!rm ./results/morph_results/* \n",
    "os.makedirs(\"./results/morph_results\", exist_ok=True)\n",
    "\n",
    "# Generate the images for the video.\n",
    "idx = 0\n",
    "for i in range(len(SEEDS)-1):\n",
    "  v1 = seed2vec(Gs, SEEDS[i])\n",
    "  v2 = seed2vec(Gs, SEEDS[i+1])\n",
    "\n",
    "  diff = v2 - v1\n",
    "  step = diff / STEPS\n",
    "  current = v1.copy()\n",
    "\n",
    "  for j in tqdm(range(STEPS), desc=f\"Seed {SEEDS[i]}\"):\n",
    "    current = current + step\n",
    "    init_random_state(Gs, 10)\n",
    "    img = generate_image(Gs, current, 1.0)\n",
    "    PIL.Image.fromarray(img, 'RGB').save(f'./results/morph_results/frame-{idx}.png')\n",
    "    idx+=1\n",
    " \n",
    "# Link the images into a video.\n",
    "#!ffmpeg -r 30 -i ./results/morph_results/frame-%d.png -vcodec mpeg4 -y movie.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3508b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -r 30 -i ./results/morph_results/frame-%d.png -vcodec libx264 -pix_fmt yuv420p -y movie.mp4\n",
    "show_local_mp4_video(\"movie.mp4\", width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976a2ec",
   "metadata": {},
   "source": [
    "## Other interesting examples.\n",
    "\n",
    "### Truncation Traversal\n",
    "\n",
    "Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. Most people choose between 0.5 and 1.0, but technically it's infinite. \n",
    "\n",
    "Below you can take one seed and look at the changes to it across any truncation amount. -1 to 1 will be pretty realistic images, but the further out you get the weirder it gets.\n",
    "\n",
    "###Options \n",
    "`--network`: Again, point this to your .pkl file.\n",
    "\n",
    "`--seed`: Pass this only one seed. Pick a favorite from your generated images.\n",
    "\n",
    "`--start`: Starting truncation value.\n",
    "\n",
    "`--stop`: Stopping truncation value. This should be larger than the start value. (Will probably break if its not).\n",
    "\n",
    "`--increment`: How much each frame should increment the truncation value. Make this really small if you want a long, slow interpolation. (stop-start/increment=total frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f34ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opensimplex\n",
    "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl -O ./pretrain/network.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py truncation-traversal --network=\"./pretrain/network.pkl\" --seed=0 --start=-2.0 --stop=2.0 --increment=0.1 --outdir=\"./results/tt\" --fps=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b9246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_local_mp4_video(\"./results/tt/truncation-traversal-seed0-start-2.0-stop2.0.mp4\", width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d41ca9",
   "metadata": {},
   "source": [
    "## Interpolations\n",
    "Interpolation is the process of generating very small changes to a vector in order to make it appear animated from frame to frame.\n",
    "\n",
    "We’ll look at two different examples of interpolation: a linear interpolation and a random noise loop.\n",
    "\n",
    "Both methods require the following options:\n",
    "\n",
    "`--network`\n",
    "\n",
    "`--walk-type`: Walk type defines the type of interpolation you want. In some cases it can also specify whether you want the z space or the w space.\n",
    "\n",
    "`--frames`: How many frames you want to produce. Use this to manage the length of your video.\n",
    "\n",
    "`--trunc`: truncation value\n",
    "\n",
    "### Linear Interpolation in Z Space     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-latent-walk --network=\"./pretrain/network.pkl\" --walk-type=\"line-z\" --seeds=0,2,5,0 --outdir=\"./results/z-walk\" #--frames 1440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfaa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_local_mp4_video(\"./results/z-walk/walk-z-line0-2-5-0-24fps.mp4\", width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae29c77",
   "metadata": {},
   "source": [
    "### Linear Interpolation in W Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2697cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-latent-walk --network=\"./pretrain/network.pkl\" --walk-type=\"line-w\" --seeds=0,2,5,0 --outdir=\"./results/w-walk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67776926",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_local_mp4_video(\"results/w-walk/walk-w-line0-2-5-0-24fps.mp4\", width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a41c61",
   "metadata": {},
   "source": [
    "### Noise Loop Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c091e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py generate-latent-walk --network=\"./pretrain/network.pkl\" --walk-type=\"noiseloop\" --start_seed=0 --outdir=\"results/noise1\" --diameter=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_local_mp4_video(\"results/noise1/walk-z-noiseloop-seed0-24fps.mp4\", width=512, height=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e94d1c",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "1. https://github.com/ArthurFDLR/GANightSky\n",
    "2. https://towardsdatascience.com/how-to-train-stylegan2-ada-with-custom-dataset-dc268ff70544\n",
    "3. https://github.com/dvschultz/stylegan2-training\n",
    "4. https://github.com/jeffheaton/t81_558_deep_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e561046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
